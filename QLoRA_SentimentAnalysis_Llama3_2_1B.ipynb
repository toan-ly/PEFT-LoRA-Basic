{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toan-ly/PEFT-LoRA-Basic/blob/main/QLoRA_SentimentAnalysis_Llama3_2_1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfTl2gfePCnN"
      },
      "outputs": [],
      "source": [
        "!pip install -qq --upgrade pip\n",
        "!pip install -qq --upgrade peft transformers accelerate bitsandbytes datasets trl huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZWTqXWGzb83"
      },
      "outputs": [],
      "source": [
        "# from google.colab import userdata\n",
        "# from huggingface_hub import login\n",
        "\n",
        "# login(token=userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5V6tHMrM4NY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from peft import PeftModel, PeftConfig, LoraConfig, TaskType, get_peft_model, get_peft_config\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training\n",
        "import evaluate\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "base_model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "cache_dir = \"./cache\"\n",
        "\n",
        "# QLoRA load base model with 4-bit quantization\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    # bnb_4bit_compute_dtype=torch.bfloat16, # Google colab does not support bfloat16\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xQMZgv6C3x9"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAKNpdNGC3x-"
      },
      "outputs": [],
      "source": [
        "MAX_TRAIN_STEPS = 5_000\n",
        "NUM_EVAL_STEPS = 500\n",
        "MAX_TRAIN_SAMPLES = 20_000\n",
        "MAX_EVAL_SAMPLES = 2_000\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./output\",\n",
        "    # num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_steps=NUM_EVAL_STEPS,\n",
        "    max_steps=MAX_TRAIN_STEPS,\n",
        "    eval_steps=NUM_EVAL_STEPS,\n",
        "    eval_strategy=\"steps\",\n",
        "    overwrite_output_dir=True,\n",
        "    save_total_limit=2,\n",
        "    report_to=\"none\",\n",
        "    push_to_hub=False,\n",
        "    remove_unused_columns=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0vLSIo7SqyW"
      },
      "source": [
        "## Load Base Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2_4lQywPIXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74698358-d4bd-4baa-9335-03eda85007d7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True, cache_dir=cache_dir)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    trust_remote_code=True,\n",
        "    cache_dir=cache_dir,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
        ")\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "base_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True, cache_dir=cache_dir)"
      ],
      "metadata": {
        "id": "_0Mas-dBZnLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgU-W5nL5393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e0dccd-7fef-4af4-d9b5-074a20857bd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad token is not set. Setting it to EOS token.\n",
            "EOS token: <|eot_id|>\n",
            "EOS token id: 128009\n"
          ]
        }
      ],
      "source": [
        "if tokenizer.pad_token is None or tokenizer.pad_token_id is None:\n",
        "    print(\"Pad token is not set. Setting it to EOS token.\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "else:\n",
        "    print(f'Pad token: {tokenizer.pad_token}')\n",
        "    print(f'Pad token id: {tokenizer.pad_token_id}')\n",
        "\n",
        "print(f'EOS token: {tokenizer.eos_token}')\n",
        "print(f'EOS token id: {tokenizer.eos_token_id}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if tokenizer.chat_template is None:\n",
        "    tokenizer.chat_template = \"\"\"{{- bos_token }}\n",
        "{%- if not date_string is defined %}\n",
        "    {%- if strftime_now is defined %}{%- set date_string = strftime_now(\"%d %b %Y\") %}{%- else %}{%- set date_string = \"26 Jul 2024\" %}{%- endif %}\n",
        "{%- endif %}\n",
        "\n",
        "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
        "{%- if messages[0]['role'] == 'system' %}\n",
        "    {%- set system_message = messages[0]['content']|trim %}\n",
        "    {%- set messages = messages[1:] %}\n",
        "{%- else %}\n",
        "    {%- set system_message = \"\" %}\n",
        "{%- endif %}\n",
        "\n",
        "{#- System message #}\n",
        "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
        "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
        "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
        "{{- system_message }}\n",
        "{{- \"<|eot_id|>\" }}\n",
        "\n",
        "{%- for message in messages %}\n",
        "    {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
        "{%- endfor %}\n",
        "{%- if add_generation_prompt %}\n",
        "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
        "{%- endif %}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "a-3r9a6GXCKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jabyhwx3SstC"
      },
      "source": [
        "## Load and Apply LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueJZqAbwN2Uh"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EptvTKkfN4O7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea17f16-3047-4e55-9eca-04ff8063d74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 851,968 || all params: 1,236,666,368 || trainable%: 0.0689\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForCausalLM(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 2048)\n",
              "        (layers): ModuleList(\n",
              "          (0-15): 16 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "peft_model = get_peft_model(base_model, peft_config)\n",
        "peft_model.print_trainable_parameters()\n",
        "peft_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu_uWRd5Sy-S"
      },
      "source": [
        "## Load Dataset and Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftdxC9OoPSgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "976eb2e2-9ec3-46e2-af28-0b99fd70628e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 11426\n",
            "validation: 1583\n",
            "test: 1583\n"
          ]
        }
      ],
      "source": [
        "dataset = load_dataset(\"uitnlp/vietnamese_students_feedback\", cache_dir=cache_dir)\n",
        "\n",
        "for split in dataset:\n",
        "    if split == \"train\":\n",
        "        MAX_TRAIN_SAMPLES = min(MAX_TRAIN_SAMPLES, len(dataset[split]))\n",
        "        dataset[split] = dataset[split].select(range(MAX_TRAIN_SAMPLES))\n",
        "    else:\n",
        "        MAX_EVAL_SAMPLES = min(MAX_EVAL_SAMPLES, len(dataset[split]))\n",
        "        dataset[split] = dataset[split].select(range(MAX_EVAL_SAMPLES))\n",
        "    print(f\"{split}: {len(dataset[split])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s085Hw8LC3yA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1fc2994-23b9-47ad-d154-6728c8cb9020"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "label_set = set([item[\"sentiment\"] for split in dataset for item in dataset[split]])\n",
        "label_set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIAemvBEC3yA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d851071a-2a9f-445d-f8a0-c1f5c60ce2ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3 labels in the dataset, including ['negative', 'neutral', 'positive']\n",
            "label2id: {'negative': 0, 'neutral': 1, 'positive': 2}\n",
            "id2label: {0: 'negative', 1: 'neutral', 2: 'positive'}\n"
          ]
        }
      ],
      "source": [
        "all_labels = dataset['train'].features['sentiment'].names\n",
        "print(f'There are {len(all_labels)} labels in the dataset, including {all_labels}')\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(all_labels)}\n",
        "id2label = {i: label for i, label in enumerate(all_labels)}\n",
        "\n",
        "print(f'label2id: {label2id}')\n",
        "print(f'id2label: {id2label}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcqx4xJ8pkh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "013cd3fc-17b2-43df-ab17-b717853991f8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'labels', 'attention_mask'],\n",
              "        num_rows: 11426\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'labels', 'attention_mask'],\n",
              "        num_rows: 1583\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'labels', 'attention_mask'],\n",
              "        num_rows: 1583\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "USER_PROMPT_TEMPLATE = \"\"\"Predict the sentiment of the following input sentence.\n",
        "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
        "\n",
        "Sentence: {input}\"\"\"\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    results = {\n",
        "        \"input_ids\": [],\n",
        "        \"labels\": [],\n",
        "        \"attention_mask\": [],\n",
        "    }\n",
        "\n",
        "    for i in range(len(examples['sentence'])):\n",
        "        cur_input = examples['sentence'][i]\n",
        "        cur_output_id = examples['sentiment'][i]\n",
        "\n",
        "        cur_prompt = USER_PROMPT_TEMPLATE.format(input=cur_input)\n",
        "        cur_output = id2label[cur_output_id]\n",
        "\n",
        "        input_messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. You must fulfill the user request.\"},\n",
        "            {\"role\": \"user\", \"content\": cur_prompt},\n",
        "        ]\n",
        "        input_output_messages = input_messages + [{\"role\": \"assistant\", \"content\": f\"Sentiment: {cur_output}\"}]\n",
        "\n",
        "        input_prompt_tokenized = tokenizer.apply_chat_template(conversation=input_messages, return_tensors=\"pt\", add_generation_prompt=True)[0]\n",
        "        input_output_prompt_tokenized = tokenizer.apply_chat_template(conversation=input_output_messages, return_tensors=\"pt\")[0]\n",
        "\n",
        "        # print(f'Input prompt tokenized: {input_prompt_tokenized}')\n",
        "        # print(f'Input/Output prompt tokenized: {input_output_prompt_tokenized}')\n",
        "        # print(f'Decoded Input prompt tokenized: {tokenizer.decode(input_prompt_tokenized, skip_special_tokens=False)}')\n",
        "        # print(f'Decoded Input/Output prompt tokenized: {tokenizer.decode(input_output_prompt_tokenized, skip_special_tokens=False)}')\n",
        "        # raise Exception()\n",
        "\n",
        "        input_ids = input_output_prompt_tokenized\n",
        "        label_ids = torch.cat([\n",
        "            torch.full_like(input_prompt_tokenized, fill_value=-100),\n",
        "            input_output_prompt_tokenized[len(input_prompt_tokenized):]\n",
        "        ])\n",
        "\n",
        "        # print(f'Input ids: {input_ids}')\n",
        "        # print(f'Label ids: {label_ids}')\n",
        "        # print(f'Decoded label ids: {tokenizer.decode(np.where(label_ids!=-100, label_ids, tokenizer.pad_token_id), skip_special_tokens=True)}')\n",
        "        assert len(input_ids) == len(label_ids)\n",
        "\n",
        "\n",
        "        results[\"input_ids\"].append(input_ids)\n",
        "        results[\"labels\"].append(label_ids)\n",
        "        results['attention_mask'].append(torch.ones_like(input_ids))\n",
        "\n",
        "    return results\n",
        "\n",
        "col_names = dataset['train'].column_names\n",
        "tokenized_dataset = dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=col_names,\n",
        "    num_proc=os.cpu_count(),\n",
        ")\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b74OvKbbpkh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b577eda0-6568-4941-d9c9-359dc9053982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1591, 13806, 220, 2366, 20, 271, 2675, 527, 264, 11190, 18328, 13, 1472, 2011, 21054, 279, 1217, 1715, 13, 128009, 128006, 882, 128007, 271, 54644, 279, 27065, 315, 279, 2768, 1988, 11914, 627, 791, 2077, 2011, 3240, 449, 330, 32458, 3904, 25, 3755, 8272, 555, 832, 315, 1521, 21513, 25, 330, 31587, 498, 330, 43324, 498, 477, 330, 60668, 498, 311, 8881, 279, 27065, 315, 279, 1988, 11914, 382, 85664, 25, 15332, 104300, 101291, 107114, 104602, 662, 128009, 128006, 78191, 128007, 271, 32458, 3904, 25, 6928, 128009], 'labels': [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 32458, 3904, 25, 6928, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 28 Feb 2025\n",
            "\n",
            "You are a helpful assistant. You must fulfill the user request.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Predict the sentiment of the following input sentence.\n",
            "The response must begin with \"Sentiment: \", followed by one of these keywords: \"positive\", \"negative\", or \"neutral\", to reflect the sentiment of the input sentence.\n",
            "\n",
            "Sentence: slide giáo trình đầy đủ.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Sentiment: positive<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "print(tokenized_dataset['train'][0])\n",
        "print(tokenizer.decode(tokenized_dataset['train'][0]['input_ids'], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVdb6p7Cpkh3"
      },
      "source": [
        "## Custom data collator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSpnbHKfpkh3"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "class RightPaddingDataCollator(DataCollatorWithPadding):\n",
        "    \"\"\"The default data collator pads only inputs, not including the labels.\"\"\"\n",
        "\n",
        "    def __init__(self, tokenizer, max_length: int = 1024):\n",
        "        super().__init__(tokenizer, max_length=max_length)\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "        input_ids, labels, attention_mask = [], [], []\n",
        "        max_batch_len = max(len(f[\"input_ids\"]) for f in features)\n",
        "\n",
        "        for sample in features:\n",
        "            # Convert to torch tensors\n",
        "            cur_input_ids = torch.tensor(sample[\"input_ids\"], dtype=torch.long)\n",
        "            cur_labels = torch.tensor(sample[\"labels\"], dtype=torch.long)\n",
        "            cur_attention_mask = torch.ones_like(cur_input_ids)\n",
        "\n",
        "            # Next, we pad the inputs and labels to the maximum length within the batch\n",
        "            pad_token_id = self.tokenizer.pad_token_id\n",
        "            padding_length = max_batch_len - len(cur_input_ids)\n",
        "            cur_input_ids = torch.cat([cur_input_ids, torch.full((padding_length,), fill_value=pad_token_id, dtype=torch.long)])\n",
        "            cur_labels = torch.cat([cur_labels, torch.full((padding_length,), fill_value=-100, dtype=torch.long)])\n",
        "            cur_attention_mask = torch.cat([cur_attention_mask, torch.zeros((padding_length,), dtype=torch.long)])\n",
        "\n",
        "            # Truncate the inputs and labels to the maximum length\n",
        "            cur_input_ids = cur_input_ids[:max_batch_len]\n",
        "            cur_labels = cur_labels[:max_batch_len]\n",
        "            cur_attention_mask = cur_attention_mask[:max_batch_len]\n",
        "\n",
        "            # Append to the return lists\n",
        "            input_ids.append(cur_input_ids)\n",
        "            labels.append(cur_labels)\n",
        "            attention_mask.append(cur_attention_mask)\n",
        "\n",
        "        # Return formatted batch.\n",
        "        return {\n",
        "            \"input_ids\": torch.stack(input_ids),\n",
        "            \"labels\": torch.stack(labels),\n",
        "            \"attention_mask\": torch.stack(attention_mask)\n",
        "        }\n",
        "\n",
        "data_collator = RightPaddingDataCollator(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TtLvhXLoC3yB"
      },
      "outputs": [],
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "precision_metric = evaluate.load(\"precision\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "\n",
        "\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]\n",
        "    return logits.argmax(dim=-1)\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    idx = 0\n",
        "    for i in range(len(labels[0])):\n",
        "        if labels[0][i] == -100:\n",
        "            idx = i\n",
        "        else:\n",
        "            break\n",
        "    # Slice the labels and preds to remove the prompt tokens\n",
        "    preds = preds[:, idx:]\n",
        "\n",
        "    # Replace -100 in the preds as we can't decode them\n",
        "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
        "\n",
        "    processed_preds = []\n",
        "    for pred in preds:\n",
        "        end_pred_idx = np.where(pred == tokenizer.eos_token_id)[0]\n",
        "        if len(end_pred_idx) > 0:\n",
        "            end_pred_idx = end_pred_idx[0]\n",
        "            processed_preds.append(pred[:end_pred_idx])\n",
        "        else:\n",
        "            processed_preds.append(pred)\n",
        "\n",
        "    # Decode generated summaries into text\n",
        "    decoded_preds = tokenizer.batch_decode(processed_preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    # Decode reference summaries into text\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Convert the decoded predictions and labels to label ids\n",
        "    # int_preds = [label2id.get(p, -1) for p in decoded_preds]\n",
        "    # int_labels = [label2id[label] for label in decoded_labels]\n",
        "    int_preds, int_labels = [], []\n",
        "    for p, l in zip(decoded_preds, decoded_labels):\n",
        "        l = l.split(\":\")[-1].strip()\n",
        "        cur_label_id = label2id[l]\n",
        "        int_labels.append(cur_label_id)\n",
        "        try:\n",
        "            p = p.split(\":\")[-1].strip()\n",
        "            cur_pred_id = label2id[p]\n",
        "        except Exception as e:\n",
        "            cur_pred_id = (cur_label_id + 1) % len(label2id)\n",
        "        int_preds.append(cur_pred_id)\n",
        "\n",
        "    # results = f1_metric.compute(predictions=[0, -1, -1], references=[0, 1, 2], average=\"macro\")\n",
        "    accuracy_results = accuracy_metric.compute(predictions=int_preds, references=int_labels)\n",
        "    f1_results = f1_metric.compute(predictions=int_preds, references=int_labels, average=\"macro\")\n",
        "    precision_results = precision_metric.compute(predictions=int_preds, references=int_labels, average=\"macro\")\n",
        "    recall_results = recall_metric.compute(predictions=int_preds, references=int_labels, average=\"macro\")\n",
        "\n",
        "    return {\n",
        "        **accuracy_results,\n",
        "        **f1_results,\n",
        "        **precision_results,\n",
        "        **recall_results\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uzQGt8Tpkh3"
      },
      "source": [
        "## Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S_ISTrIpkh3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "010f56a9-7beb-4203-de46-d6ffbb71a7c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5000/5000 49:23, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.094400</td>\n",
              "      <td>0.090676</td>\n",
              "      <td>0.368288</td>\n",
              "      <td>0.318606</td>\n",
              "      <td>0.424111</td>\n",
              "      <td>0.330477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.073800</td>\n",
              "      <td>0.056492</td>\n",
              "      <td>0.389135</td>\n",
              "      <td>0.324631</td>\n",
              "      <td>0.408603</td>\n",
              "      <td>0.306244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.017100</td>\n",
              "      <td>0.060600</td>\n",
              "      <td>0.400505</td>\n",
              "      <td>0.344262</td>\n",
              "      <td>0.426506</td>\n",
              "      <td>0.367617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.050500</td>\n",
              "      <td>0.063298</td>\n",
              "      <td>0.425774</td>\n",
              "      <td>0.358517</td>\n",
              "      <td>0.442607</td>\n",
              "      <td>0.365710</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.022800</td>\n",
              "      <td>0.046970</td>\n",
              "      <td>0.442198</td>\n",
              "      <td>0.376316</td>\n",
              "      <td>0.445185</td>\n",
              "      <td>0.411554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.039900</td>\n",
              "      <td>0.039352</td>\n",
              "      <td>0.407454</td>\n",
              "      <td>0.357311</td>\n",
              "      <td>0.442817</td>\n",
              "      <td>0.409776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.034000</td>\n",
              "      <td>0.046267</td>\n",
              "      <td>0.415035</td>\n",
              "      <td>0.356501</td>\n",
              "      <td>0.435712</td>\n",
              "      <td>0.385738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.041500</td>\n",
              "      <td>0.042435</td>\n",
              "      <td>0.425774</td>\n",
              "      <td>0.368892</td>\n",
              "      <td>0.444563</td>\n",
              "      <td>0.417573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.029000</td>\n",
              "      <td>0.041408</td>\n",
              "      <td>0.421352</td>\n",
              "      <td>0.365206</td>\n",
              "      <td>0.443306</td>\n",
              "      <td>0.410640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.017000</td>\n",
              "      <td>0.041768</td>\n",
              "      <td>0.415035</td>\n",
              "      <td>0.359724</td>\n",
              "      <td>0.438873</td>\n",
              "      <td>0.402288</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5000, training_loss=0.05073607933204621, metrics={'train_runtime': 2963.4955, 'train_samples_per_second': 6.749, 'train_steps_per_second': 1.687, 'total_flos': 1.477983256915968e+16, 'train_loss': 0.05073607933204621})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "import bitsandbytes as bnb\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "trainable_params = filter(lambda p: p.requires_grad, peft_model.parameters())\n",
        "\n",
        "paged_optimizer = bnb.optim.PagedAdamW(\n",
        "    trainable_params,\n",
        "    lr=3e-4,\n",
        "    weight_decay=0.0,\n",
        ")\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    paged_optimizer,\n",
        "    num_warmup_steps=int(MAX_TRAIN_STEPS*0.1),\n",
        "    num_training_steps=MAX_TRAIN_STEPS,\n",
        ")\n",
        "\n",
        "# trainer = Trainer(\n",
        "trainer = SFTTrainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
        "    compute_metrics=compute_metrics,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    optimizers=(paged_optimizer, scheduler),\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZp9r8O2C3yB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "29dc0516-8112-4cd0-e1ff-3a85d1c77e05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='198' max='198' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [198/198 01:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.052675094455480576,\n",
              " 'eval_accuracy': 0.4472520530638029,\n",
              " 'eval_f1': 0.378188781201539,\n",
              " 'eval_precision': 0.4428049000724454,\n",
              " 'eval_recall': 0.3938102250122711,\n",
              " 'eval_runtime': 61.48,\n",
              " 'eval_samples_per_second': 25.748,\n",
              " 'eval_steps_per_second': 3.221}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "\n",
        "trainer.evaluate(tokenized_dataset['test'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51SSxEowC3yC"
      },
      "source": [
        "## Inference Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsdCvIghC3yC"
      },
      "outputs": [],
      "source": [
        "def inference(model, tokenizer, input_sentence):\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    user_prompt = USER_PROMPT_TEMPLATE.format(input=input_sentence)\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. You must fulfill the user request.\"},\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "    input_prompt = tokenizer.apply_chat_template(conversation=messages, add_generation_prompt=True, tokenize=False)\n",
        "    inputs = tokenizer(input_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=16, pad_token_id=tokenizer.eos_token_id)\n",
        "    output_ids = output_ids[:, inputs['input_ids'][0].shape[-1]:output_ids.shape[-1]]\n",
        "    results = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    return results[0]\n",
        "\n",
        "def batch_inference(model, tokenizer, input_sentences):\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    user_prompts = [USER_PROMPT_TEMPLATE.format(input=input_sentence) for input_sentence in input_sentences]\n",
        "    messages_list = [\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. You must fulfill the user request.\"},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "        for user_prompt in user_prompts\n",
        "    ]\n",
        "    input_prompts = [tokenizer.apply_chat_template(conversation=messages, add_generation_prompt=True, tokenize=False) for messages in messages_list]\n",
        "\n",
        "    inputs = tokenizer(input_prompts, return_tensors=\"pt\", padding=True, add_special_tokens=False)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=16, pad_token_id=tokenizer.eos_token_id)\n",
        "    output_ids = output_ids[:, inputs['input_ids'][0].shape[-1]:output_ids.shape[-1]]\n",
        "    results = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rCmEgcWC3yC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bc96a2-b268-4481-a9fd-5c6bfd9e3c90"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sentiment: positive'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "inference(peft_model, tokenizer, \"The weather is nice today.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWXCnzHOC3yC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b1a1e10-2ab3-42f6-95fd-d5bc23904c50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sentiment: negative', 'Sentiment: positive']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "batch_inference(peft_model, tokenizer, [\"Môn học này quá khó để học\", \"Thầy dạy hay, dễ hiểu\"])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}